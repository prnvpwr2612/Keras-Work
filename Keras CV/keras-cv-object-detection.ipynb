{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":6102,"sourceType":"modelInstanceVersion","modelInstanceId":4645}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n<p style=\"text-align: justify;\">\nWelcome to this comprehensive guide on object detection using the Keras-CV library. Object detection is a crucial computer vision task that involves identifying and locating objects within images. \n</p>\n<p style=\"text-align: justify;\">\nImage object detection is a complex task in the realm of computer vision, posing unique challenges that demand sophisticated solutions. Unlike single-output tasks, object detection requires training models to predict both bounding box coordinates and class labels, adding a layer of intricacy to the learning process. Additionally, the scarcity of labeled examples for object detection further intensifies the difficulty of training accurate and robust models.\n</p>\n<p style=\"text-align: justify;\">\nObject detection involves identifying and precisely localizing multiple objects within an image. This dual nature of the task—predicting bounding box coordinates and associated class labels—introduces a higher level of complexity compared to single-output tasks like image classification. Achieving accurate localization and classification simultaneously demands advanced architectures and careful training strategies.\nKeras-CV simplifies the development of object detection models, providing pre-trained models and powerful utilities.\n</p>\n<p style=\"text-align: justify;\">\nIn this notebook, we will explore step-by-step procedures for performing object detection using Keras-CV, starting from dataset preparation to model training and evaluation. We'll use the COCO 2017 dataset as an example, but the techniques can be applied to custom datasets with minimal adjustments.\n</p>\n\n**If you think this notebook could be a resource for others, consider giving it an upvote for better discoverability!**","metadata":{}},{"cell_type":"markdown","source":"## Online Demo\n<p style=\"text-align: justify;\">\nYou can work with online demo in the following address:\n<a\nhref='https://hamiddamadi.ir/app/imageObjectDetection'\ntarget='_blank'\nrel=\"noreferrer\"\n>\nhttps://hamiddamadi.ir/app/imageObjectDetection.\n</a>\n</p>","metadata":{}},{"cell_type":"markdown","source":"## Prerequisites\n<p style=\"text-align: justify;\">\nMake sure you have the required libraries installed before running the notebook.\n</p>\n<p style=\"text-align: justify;\">\nThis code cell installs the pycocotools package using the pip package manager. 'pycocotools' is a Python API that provides tools for working with the COCO (Common Objects in Context) dataset. This dataset is commonly used for object detection, segmentation, and captioning tasks in computer vision. Installing this package allows you to access and manipulate the COCO dataset in your code.\n</p>","metadata":{}},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Required Libraries and Modules\nThis code cell imports several Python libraries and modules:\n\n* **keras_cv**: It seems to be a library related to Keras and computer vision tasks.\n* **cv2**: The OpenCV library for computer vision tasks.\n* **tensorflow** (imported as tf): TensorFlow, a popular machine learning library.\n* **os**: The OS module for interacting with the operating system.\n* **numpy** (imported as np): NumPy, a library for numerical operations in Python.\n* **pandas** (imported as pd): Pandas, a library for data manipulation and analysis.\n* **pycocotools**: The Python API for working with the COCO (Common Objects in Context) dataset.","metadata":{}},{"cell_type":"code","source":"import keras_cv \nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport pandas as pd\nimport pycocotools","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constants\nIMAGE_SIZE = (640, 640)\nAUTOTUNE = tf.data.AUTOTUNE # Used for configuring parallelism in TensorFlow's data loading pipeline\nBBOX_FORMAT = 'xywh' # Indicating the format of bounding boxes\nBATCH_SIZE = 4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Dataset For KerasCV Models\n<p style=\"text-align: justify;\">\nThis section is about prepartion of dataset to adjust with keras_cv models' dataset format. We'll use the COCO 2017 dataset as an example, but the techniques can be applied to custom datasets with minimal adjustments.\n</p>\n<p style=\"text-align: justify;\">\nFor KerasCV to work with your bounding boxes, you need to convert them into a specific dictionary format. The following requirements outline the necessary structure.\n\n```\nbounding_boxes = {\n    # num_boxes may be a Ragged dimension\n    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n    'classes': Tensor(shape=[batch, num_boxes])\n}\n```\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"text-align: justify;\">\nKEYS_TO_KEEP: This is a list containing the class labels that we want to keep from the COCO dataset. We will filter out annotations corresponding to classes not present in this list.\n</p>","metadata":{}},{"cell_type":"code","source":"# Load COCO annotations (adjust the paths accordingly)\ncoco_annotation_file = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_train2017.json\"\ncoco_image_dir_train = \"/kaggle/input/coco-2017-dataset/coco2017/train2017\"\ncoco_train = pycocotools.coco.COCO(coco_annotation_file)\nKEYS_TO_KEEP = [1, 2, 3, 4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: justify;\">\nThere are a few bounding box formats. For more info on supported bounding box formats, visit\n<a\nhref='https://keras.io/api/keras_cv/bounding_box/formats/'\ntarget='_blank'\nrel=\"noreferrer\"\n>\nhttps://keras.io/api/keras_cv/bounding_box/formats/.\n</a>\n</p>\n<p style=\"text-align: justify;\">\nFirst, it is possible to perform format conversion between any two pairs:\n</p>\n\n```\ndef normalize_bbox(inputs):\n    image = inputs[\"image\"]\n    normalized_bbox = keras_cv.bounding_box.convert_format(\n        inputs[\"objects\"][\"bbox\"],\n        images=image,\n        source=\"xywh\",\n        target=BBOX_FORMAT,\n    )\n    return normalized_bbox\n```\n\n<p style=\"text-align: justify;\">\nThis function is responsible for normalizing the bounding box coordinates to the required format. It takes the bounding box coordinates and the image dimensions as input and returns the normalized coordinates.\n</p>\n<p style=\"text-align: justify;\">\nNote that our source and target bounding_box_format are the same, so, we don't use this function. \n</p>\n<p style=\"text-align: justify;\">\nNow, we define various functions that will be used throughout the notebook for preparing the dataset. These include:\n</p>\n\n* **get_annotations_for_image**: This function retrieves the annotations for a specific image ID from the COCO dataset. It filters out annotations based on the classes specified in the KEYS_TO_KEEP list.\n\n* **decode_and_resize**: This function reads and decodes images from file paths. It is used to preprocess images before feeding them into the model.\n\n* **load_coco_dataset**: This function loads the COCO dataset, retrieves annotations for each image, and filters out unwanted classes using the get_annotations_for_image function.\n\n* **load_dataset**: This function loads and preprocesses a single dataset sample. It reads an image file, decodes it, and prepares the bounding box and class label information in the required format.\n\n* **make_dataset**: This function creates a TensorFlow dataset by loading and preprocessing a specified range of images from the COCO dataset. It utilizes the load_coco_dataset and load_dataset functions to prepare the dataset for training.","metadata":{}},{"cell_type":"code","source":"def get_annotations_for_image(image_id, coco):\n    # Get annotations for the specified image ID from COCO dataset\n    annotation_ids = coco.getAnnIds(imgIds=image_id)\n    annotations = coco.loadAnns(annotation_ids)\n\n    # Placeholder for bounding box and label information\n    bounding_boxes = []\n    labels = []\n\n    for annotation in annotations:\n        bbox = annotation['bbox']  # Format: [x, y, width, height]\n        label = annotation['category_id'] \n        if(label in KEYS_TO_KEEP):\n            converted_label = KEYS_TO_KEEP.index(label)\n            bounding_boxes.append(bbox)\n            labels.append(converted_label)\n\n    return {'boxes': bounding_boxes, 'labels': labels}\n\n\ndef decode_and_resize(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    return img\n\n\ndef load_coco_dataset(image_ids, coco, coco_image_dir):\n    images = []\n    boxes_list = []\n    labels_list = []\n\n    for imgid in image_ids:\n        img = coco.loadImgs(imgid)[0]\n        img_name = img['file_name'].strip()\n        img_path = os.path.join(coco_image_dir, img_name)\n\n        if os.path.isfile(img_path) and os.path.getsize(img_path):\n            annotations = get_annotations_for_image(imgid, coco)\n            if len(annotations['boxes']) > 0 and len(annotations['labels']) > 0:\n                images.append(img_path)\n                boxes_list.append(annotations['boxes'])\n                labels_list.append(annotations['labels'])\n    \n    return images, boxes_list, labels_list\n\ndef load_dataset(image_path, classes, bbox):\n    # Read Image\n    image = decode_and_resize(image_path)\n    bounding_boxes = {\n        \"classes\": tf.cast(classes, dtype=tf.float32),\n        \"boxes\": bbox,\n    }\n    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n\ndef make_dataset(m,n, coco, coco_image_dir):\n    image_ids = coco.getImgIds()[m:n]\n    image_paths, bbox, classes = load_coco_dataset(image_ids, coco, coco_image_dir)\n    bbox = tf.ragged.constant(bbox)\n    classes = tf.ragged.constant(classes)\n    image_paths = tf.ragged.constant(image_paths)\n\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))\n    dataset = dataset.map(load_dataset, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.ragged_batch(BATCH_SIZE, drop_remainder=True)\n    dataset = dataset.shuffle(8 * BATCH_SIZE)\n    dataset = dataset.apply(tf.data.experimental.ignore_errors())\n    dataset = dataset.prefetch(AUTOTUNE)\n    \n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have defined our constants and functions, let's use them to create the training and validation datasets.\n\n* **train_dataset**: This dataset is created using the make_dataset function, specifying the range of images from 0 to 10,000. It will be used for training the object detection model.\n\n* **val_dataset**: Similarly, the validation dataset is created by specifying the range of images from 10,000 to 13,000. This dataset will be used to evaluate the model's performance on unseen data.\n\nThese datasets are crucial for training and evaluating the model's ability to detect objects in different images. The data is shuffled, batched, and preprocessed to ensure optimal training conditions.","metadata":{}},{"cell_type":"code","source":"train_dataset = make_dataset(0,500, coco_train, coco_image_dir_train)\nval_dataset = make_dataset(500,700, coco_train, coco_image_dir_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we refine the class mapping to suit our specific needs for the object detection model.\n\n* **get_class_mapping**: The function retrieves the original class mapping from the COCO dataset. This mapping includes category IDs and their corresponding names.\n* **original_dict**: The original class mapping obtained from the COCO dataset.\n* **key_mapping**: A mapping is created to convert original keys (category IDs) to a desired range [0, len(KEYS_TO_KEEP)). This is important for aligning the class indices with our reduced set of classes.\n* **class_mapping**: The final class mapping is generated by applying the key_mapping to the original dictionary. Only the classes present in KEYS_TO_KEEP are retained, and their IDs are adjusted to start from 0.","metadata":{}},{"cell_type":"code","source":" def get_class_mapping():\n    categories = coco_train.loadCats(coco_train.getCatIds())\n    class_mapping = {category['id']: category['name'] for category in categories}\n    return class_mapping\n\noriginal_dict = get_class_mapping()\n\n# Create a mapping to convert original keys to the desired range [0, len(KEYS_TO_KEEP))\nkey_mapping = {key: i for i, key in enumerate(sorted(KEYS_TO_KEEP))}\n\nclass_mapping = {key_mapping[key]: value for key, value in original_dict.items() if key in KEYS_TO_KEEP}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization and Data Augmentation\n\n<p style=\"text-align: justify;\">\nHere, we define a function visualize_dataset to display a gallery of images with their corresponding bounding boxes. This can be useful for visually inspecting the dataset and verifying the correctness of the preprocessing steps.\n<p>\nInputs:\n\n* **inputs**: Dataset input containing images and bounding boxes.\n* **rows**: Number of rows in the visualization grid.\n* **cols**: Number of columns in the visualization grid.\n    \n<p style=\"text-align: justify;\">\nThe function extracts a batch from the dataset (next(iter(inputs.take(1)))). It then retrieves images and bounding boxes from the batch. Finally, the plot_bounding_box_gallery function from keras_cv.visualization is used to visualize the images with bounding boxes, applying the specified parameters.\n</p>\n<p style=\"text-align: justify;\">\nThis visualization aids in understanding how the data is processed and prepared for training.\n</p>","metadata":{}},{"cell_type":"code","source":"def visualize_dataset(inputs, rows, cols):\n    inputs = next(iter(inputs.take(1)))\n    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n    keras_cv.visualization.plot_bounding_box_gallery(\n        images,\n        value_range=(0,255),\n        rows=rows,\n        cols=cols,\n        y_true=bounding_boxes,\n        scale=5,\n        font_scale=0.7,\n        bounding_box_format=BBOX_FORMAT,\n        class_mapping=class_mapping,\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: justify;\">\nThis code snippet visualizes a sample from the training dataset. It extracts a batch from the dataset, retrieves images and their corresponding bounding boxes, and then plots them using the plot_bounding_box_gallery function from keras_cv.visualization. Adjust the rows and cols parameters to change the layout of the gallery.\n</p>","metadata":{}},{"cell_type":"code","source":"visualize_dataset(train_dataset, rows=2, cols=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: justify;\">\nThis code snippet demonstrates the application of data augmentation to the training dataset using a list of augmenters. The create_augmenter_fn function is defined to create an augmenter function that applies each augmenter in the list sequentially. The resulting augmenter function is then applied to the training dataset using the map function. Adjust the augmenters and their parameters based on your specific requirements.\n</p>","metadata":{}},{"cell_type":"code","source":"augmenters = [\n    keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=BBOX_FORMAT),\n    keras_cv.layers.JitteredResize(\n        target_size=IMAGE_SIZE, scale_factor=(0.75, 1.3), bounding_box_format=BBOX_FORMAT\n    ),\n]\n\n\ndef create_augmenter_fn(augmenters):\n    def augmenter_fn(inputs):\n        for augmenter in augmenters:\n            inputs = augmenter(inputs)\n        return inputs\n\n    return augmenter_fn\n\n\naugmenter_fn = create_augmenter_fn(augmenters)\n\ntrain_dataset = train_dataset.map(augmenter_fn, num_parallel_calls=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_dataset(train_dataset, rows=2, cols=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of using JitteredResize, let's use the deterministic keras_cv.layers.Resizing() layer.","metadata":{}},{"cell_type":"code","source":"inference_resizing = keras_cv.layers.Resizing(\n    IMAGE_SIZE[0], IMAGE_SIZE[1], bounding_box_format=BBOX_FORMAT, pad_to_aspect_ratio=True\n)\nval_dataset = val_dataset.map(inference_resizing, num_parallel_calls=AUTOTUNE)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_dataset(val_dataset, rows=2, cols=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Model\n<p style=\"text-align: justify;\">\nThis code snippet shows how to create a YOLOv8Detector model from a preset. The model is instantiated with the yolo_v8_m backbone, the specified bounding box format, and the number of classes obtained from the class_mapping dictionary. Adjust the backbone and other parameters based on your model requirements.\n</p>","metadata":{}},{"cell_type":"code","source":"model = keras_cv.models.YOLOV8Detector.from_preset(\n    \"yolo_v8_m_backbone\",\n    bounding_box_format=BBOX_FORMAT,\n    num_classes=4\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: justify;\">\nIn this code snippet, the base learning rate is set to 0.005, and an SGD optimizer is created with a momentum of 0.9 and a global clipnorm of 10.0. The global clipnorm is crucial for stability in object detection tasks. You can adjust the learning rate and other parameters based on your specific requirements.\n</p>","metadata":{}},{"cell_type":"code","source":"base_lr = 0.005\noptimizer = tf.keras.optimizers.SGD(\n    learning_rate=base_lr, momentum=0.9, global_clipnorm=10.0\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: justify;\">\nHere, the YOLOv8 model is compiled using binary crossentropy for classification loss and the Complete IoU (CIoU) loss for bounding box regression. The previously defined SGD optimizer is used. You can customize the choice of losses and optimizer based on your specific use case.\n</p>","metadata":{}},{"cell_type":"code","source":"model.compile(\n    classification_loss=\"binary_crossentropy\",\n    box_loss=\"ciou\",\n    optimizer=optimizer,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and Saving the Model\n<p style=\"text-align: justify;\">\nWe need to extract the inputs from the preprocessing dictionary and get them ready to be fed into the model.\n</p>","metadata":{}},{"cell_type":"code","source":"def dict_to_tuple(inputs):\n    return inputs[\"images\"], keras_cv.bounding_box.to_dense(\n        inputs[\"bounding_boxes\"], max_boxes=32\n    )\n\n\ntrain_dataset = train_dataset.map(dict_to_tuple, num_parallel_calls=AUTOTUNE)\nval_dataset = val_dataset.map(dict_to_tuple, num_parallel_calls=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: justify;\">\nThis callback is used to evaluate metrics on the validation dataset during training. It's specifically designed for tasks like object detection with bounding box annotations in COCO format. The bounding_box_format parameter should be set according to the format used in your dataset. We also save our model when the mAP score improves.\n</p>","metadata":{}},{"cell_type":"code","source":"class EvaluateCOCOMetricsCallback(tf.keras.callbacks.Callback):\n    def __init__(self, data, save_path):\n        super().__init__()\n        self.data = data\n        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n            bounding_box_format=BBOX_FORMAT,\n            evaluate_freq=1e9,\n        )\n\n        self.save_path = save_path\n        self.best_map = -1.0\n\n    def on_epoch_end(self, epoch, logs):\n        self.metrics.reset_state()\n        for batch in self.data:\n            images, y_true = batch[0], batch[1]\n            y_pred = self.model.predict(images, verbose=0)\n            self.metrics.update_state(y_true, y_pred)\n\n        metrics = self.metrics.result(force=True)\n        logs.update(metrics)\n\n        current_map = metrics[\"MaP\"]\n        if current_map > self.best_map:\n            self.best_map = current_map\n            self.model.save(self.save_path)  # Save the model when mAP improves\n\n        return logs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: justify;\">\nThis code snippet trains the model using the fit method. The training dataset (train_dataset) is passed to the fit method, and the training process runs for a specified number of epochs (Run for 10-35~ epochs to achieve good scores). The EvaluateCOCOMetricsCallback is added to the list of callbacks, which allows the evaluation of COCO metrics on the validation dataset during training.\n</p>","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=30,\n    callbacks=[EvaluateCOCOMetricsCallback(val_dataset, \"model.keras\")]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_train_history():\n    \"\"\"\n    Visualize the training history (accuracy and loss).\n\n    Parameters:\n        history (tf.keras.callbacks.History): Training history.\n    \"\"\"\n    plt.figure(figsize=(12, 4))\n    # Plot training & validation loss values\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n\n    plt.tight_layout()\n    plt.show()\n\nvisualize_train_history()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating the Model","metadata":{}},{"cell_type":"code","source":"# Load COCO annotations (adjust the paths accordingly)\ncoco_annotation_file_test = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json\"\ncoco_image_dir_test = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\ncoco_test = pycocotools.coco.COCO(coco_annotation_file_test)\n\ntest_dataset = make_dataset(0,400, coco_test, coco_image_dir_test)\ntest_dataset = test_dataset.map(inference_resizing, num_parallel_calls=AUTOTUNE)\ntest_dataset = test_dataset.map(dict_to_tuple, num_parallel_calls=AUTOTUNE)\n\nmodel.evaluate(test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n<p style=\"text-align: justify;\">\nYou may need to configure your MultiClassNonMaxSuppression operation to achieve visually appealing results.\n</p>","metadata":{}},{"cell_type":"code","source":"model.prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n    bounding_box_format=BBOX_FORMAT,\n    from_logits=True,\n    iou_threshold=0.2,\n    confidence_threshold=0.6,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and preprocess image\n# image = cv2.imread(\"/kaggle/input/coco-2017-dataset/coco2017/test2017/000000000001.jpg\")\nfilepath = tf.keras.utils.get_file(origin=\"https://stackabuse.s3.amazonaws.com/media/object-detection-with-imageai-python-1.jpg\")\nimage = tf.keras.utils.load_img(filepath)\nimage = np.array(image)\nimage_batch = inference_resizing([image])\n# Run prediction\ny_pred = model.predict(image_batch)\nprint(y_pred)\n# y_pred is a bounding box Tensor:\n# {\"classes\": ..., boxes\": ...}\nkeras_cv.visualization.plot_bounding_box_gallery(\n    image_batch,\n    value_range=(0, 255),\n    rows=1,\n    cols=1,\n    y_pred=y_pred,\n    scale=5,\n    font_scale=0.7,\n    bounding_box_format=BBOX_FORMAT,\n    class_mapping=class_mapping\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}